# -*- coding: utf-8 -*-
"""Jerome Dilated2 Max Keras UNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gqgIslEVA2Rr4uutcqTk6weiNZAieQU_
"""

"""# About this model
In this iteration of the project we use the UNet architecture. This code is taken and adapted from https://github.com/milesial/Pytorch-UNet. 

The loss is binary cross entropy with weights calculated from the dataset.
"""

from google.colab import drive

import os
import re
import glob
import scipy.ndimage

import numpy as np
import matplotlib.pyplot as plt

from PIL import Image
from keras.models import compile, fit, fit_generator, predict, predict_generator
from keras.layers import Dropout, Conv2D, UpSampling2D, MaxPooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TerminateOnNaN
from keras import backend as K
import matplotlib.image as mpimg
from sklearn.model_selection import train_test_split

prefix = "/content/drive"
drive.mount(prefix, force_remount=True)
# needed accesses to the data and more importantly has added it to his drive under /My Drive/Colab Notebooks/road_segmentation/
prefix = prefix + '/My Drive/Colab Notebooks/road_segmentation/'


# Paths for data and models
path_to_models = prefix + 'models/jerome/'
path_to_results = prefix + "results/jerome/"

# Load all images
imgs, gt_images = load_data(prefix)

#create overlapping 128x128 patches
img_patches, gt_patches = make_patches(imgs, gt_imgs, patch_size1 = 128, stride1= 16)

#splie the data into 80% train and 20% validation sets
X_train, X_test, y_train, y_test = train_test_split(img_patches, gt_patches, test_size=0.2, random_state=42)

#create generators that yield batches of 32 patches at a time
train_generator, test_generator = create_train_test_generators(X_train, y_train, X_test, y_test, batch_size = 32, rotations_range = 45, \
				padding = 'reflect', hori_flip = True, vert_flip = True, brightness_range = [0.2,1.0])


model = 0
if(not pre_loaded_weights):
	model = unet_4_layers(pretrained_weights = None, input_size = (128,128,3), nb_out_channels = 32)
	model_checkpoint = ModelCheckpoint(path_to_models + 'keras_model_patch128.hdf5', monitor='val_f1_m', verbose=1,\
                          save_best_only=True, mode='max', period=1)
	# fits the model on batches with real-time data augmentation:
	terminate = TerminateOnNaN()
	early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='min', restore_best_weights=True)
	reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=5, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=0) 
	
	data_for_graph = model.fit_generator(train_generator, steps_per_epoch=int(img_patches.shape[0] * 0.8 / 32), \
                                     epochs=200, callbacks=[model_checkpoint, reduce_lr, terminate, early_stop], validation_data=test_generator, \
                                     validation_steps= int(img_patches.shape[0] * 0.2 / 32)+1, validation_freq=1)
else:
	dependencies = {
    'f1_m': f1_m,
    'precision_m': precision_m,
    'recall_m': recall_m,
    'nnztr_m' : nnztr_m,
    'nnzte_m' : nnzte_m,
	}

	model = load_model(path_to_models + 'keras_patch128_dilated_deeper.hdf5', custom_objects=dependencies)

#saves all the 50 predictions on the test set in the results folder
save_all_results_patches(path_to_results, net, patch_size=128, stride=8, glob_remove=False,threshold=16*16*2)

#Create submission csv on the test set

